{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from tqdm import tqdm # for showing progress\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing dataset\n",
    "data = pd.read_csv(\"training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BIOGRAPHIES PERSONALITIES PEOPLE\n",
      "top word counts\n",
      "[('year', 249), ('presid', 198), ('yeltsin', 140), ('stat', 133), ('minist', 126), ('hospit', 124), ('offic', 122), ('lead', 121), ('told', 117), ('report', 113), ('work', 95), ('time', 95), ('fami', 93), ('peopl', 91), ('day', 91), ('polic', 91), ('polit', 90), ('doct', 86), ('party', 85), ('world', 83)]\n",
      "\n",
      "top occurences\n",
      "[('year', 0.73), ('told', 0.49), ('stat', 0.45), ('offic', 0.42), ('lead', 0.41), ('presid', 0.38), ('day', 0.35), ('time', 0.35), ('early', 0.34), ('report', 0.34), ('work', 0.34), ('world', 0.34), ('month', 0.31), ('wednesday', 0.31), ('hospit', 0.31), ('countr', 0.3), ('fami', 0.29), ('thursday', 0.29), ('peopl', 0.28), ('minist', 0.28)]\n",
      "\n",
      "\n",
      "DEFENCE\n",
      "top word counts\n",
      "[('nato', 511), ('stat', 410), ('milit', 406), ('forc', 381), ('minist', 364), ('defend', 357), ('offic', 317), ('russia', 299), ('presid', 238), ('plan', 235), ('year', 233), ('told', 230), ('countr', 228), ('govern', 224), ('arm', 221), ('troop', 213), ('chin', 208), ('russian', 198), ('unit', 197), ('army', 194)]\n",
      "\n",
      "top occurences\n",
      "[('stat', 0.6), ('minist', 0.57), ('milit', 0.57), ('told', 0.55), ('offic', 0.54), ('defend', 0.53), ('forc', 0.51), ('year', 0.5), ('presid', 0.47), ('countr', 0.45), ('report', 0.44), ('plan', 0.43), ('govern', 0.41), ('unit', 0.39), ('nation', 0.38), ('secur', 0.37), ('war', 0.37), ('lead', 0.37), ('gener', 0.36), ('foreign', 0.36)]\n",
      "\n",
      "\n",
      "FOREX MARKETS\n",
      "top word counts\n",
      "[('dollar', 3358), ('bank', 2178), ('rate', 1755), ('trad', 1678), ('market', 1668), ('percent', 1336), ('currenc', 1292), ('deal', 1208), ('mark', 1191), ('yen', 962), ('cent', 953), ('clos', 932), ('high', 835), ('econom', 742), ('low', 675), ('year', 647), ('exchang', 635), ('foreign', 585), ('point', 558), ('end', 531)]\n",
      "\n",
      "top occurences\n",
      "[('dollar', 0.81), ('bank', 0.7), ('rate', 0.68), ('market', 0.64), ('trad', 0.6), ('currenc', 0.59), ('cent', 0.5), ('clos', 0.48), ('deal', 0.48), ('newsroom', 0.48), ('exchang', 0.46), ('percent', 0.45), ('mark', 0.44), ('foreign', 0.4), ('low', 0.4), ('high', 0.38), ('end', 0.35), ('expect', 0.34), ('year', 0.34), ('tuesday', 0.33)]\n",
      "\n",
      "\n",
      "ARTS CULTURE ENTERTAINMENT\n",
      "top word counts\n",
      "[('year', 156), ('film', 153), ('million', 102), ('stat', 83), ('work', 81), ('offic', 80), ('show', 72), ('releas', 72), ('peopl', 70), ('music', 69), ('told', 66), ('star', 65), ('world', 64), ('award', 63), ('time', 62), ('act', 60), ('day', 58), ('includ', 57), ('unit', 53), ('direct', 51)]\n",
      "\n",
      "top occurences\n",
      "[('year', 0.57), ('offic', 0.39), ('time', 0.38), ('day', 0.36), ('told', 0.36), ('film', 0.35), ('stat', 0.35), ('peopl', 0.34), ('includ', 0.32), ('million', 0.32), ('show', 0.31), ('star', 0.3), ('nation', 0.3), ('work', 0.29), ('week', 0.28), ('made', 0.28), ('part', 0.28), ('report', 0.26), ('direct', 0.26), ('unit', 0.26)]\n",
      "\n",
      "\n",
      "HEALTH\n",
      "top word counts\n",
      "[('year', 253), ('health', 248), ('stat', 241), ('diseas', 184), ('tobacc', 169), ('drug', 165), ('peopl', 151), ('canc', 148), ('study', 143), ('percent', 139), ('smok', 138), ('million', 126), ('report', 124), ('case', 123), ('research', 119), ('compan', 116), ('group', 103), ('govern', 102), ('child', 100), ('commit', 98)]\n",
      "\n",
      "top occurences\n",
      "[('year', 0.58), ('stat', 0.54), ('health', 0.53), ('peopl', 0.41), ('report', 0.36), ('diseas', 0.36), ('percent', 0.33), ('time', 0.32), ('high', 0.31), ('treat', 0.29), ('nation', 0.29), ('case', 0.28), ('govern', 0.28), ('drug', 0.28), ('thursday', 0.28), ('unit', 0.28), ('million', 0.28), ('group', 0.28), ('offic', 0.28), ('medic', 0.27)]\n",
      "\n",
      "\n",
      "IRRELEVANT\n",
      "top word counts\n",
      "[('percent', 5945), ('year', 5237), ('million', 4740), ('trad', 3678), ('market', 3556), ('shar', 3294), ('compan', 3190), ('stat', 3160), ('bank', 3095), ('pric', 2611), ('govern', 2328), ('rate', 2263), ('offic', 2208), ('month', 2191), ('billion', 2164), ('expect', 2090), ('cent', 2049), ('week', 1928), ('high', 1883), ('minist', 1873)]\n",
      "\n",
      "top occurences\n",
      "[('year', 0.47), ('percent', 0.37), ('million', 0.35), ('stat', 0.35), ('newsroom', 0.34), ('market', 0.31), ('compan', 0.3), ('month', 0.28), ('trad', 0.28), ('expect', 0.26), ('shar', 0.25), ('offic', 0.24), ('end', 0.24), ('pric', 0.23), ('report', 0.23), ('week', 0.22), ('high', 0.22), ('govern', 0.22), ('wednesday', 0.21), ('thursday', 0.21)]\n",
      "\n",
      "\n",
      "SPORTS\n",
      "top word counts\n",
      "[('play', 1491), ('win', 1054), ('game', 947), ('year', 938), ('world', 928), ('match', 925), ('champ', 880), ('cup', 786), ('team', 734), ('scor', 707), ('final', 703), ('austral', 666), ('socc', 617), ('leagu', 609), ('point', 603), ('beat', 588), ('time', 539), ('lead', 528), ('season', 510), ('open', 502)]\n",
      "\n",
      "top occurences\n",
      "[('play', 0.5), ('match', 0.43), ('win', 0.4), ('year', 0.36), ('socc', 0.35), ('champ', 0.35), ('world', 0.34), ('game', 0.33), ('final', 0.33), ('team', 0.31), ('won', 0.29), ('saturday', 0.29), ('cup', 0.29), ('time', 0.28), ('day', 0.27), ('scor', 0.27), ('lead', 0.27), ('sunday', 0.27), ('beat', 0.25), ('result', 0.25)]\n",
      "\n",
      "\n",
      "SCIENCE AND TECHNOLOGY\n",
      "top word counts\n",
      "[('spac', 141), ('mir', 110), ('shuttl', 83), ('scient', 77), ('year', 73), ('station', 70), ('crew', 68), ('mission', 67), ('nasa', 62), ('research', 61), ('system', 60), ('control', 55), ('russian', 55), ('day', 49), ('offic', 49), ('astronaut', 48), ('work', 46), ('clon', 42), ('stat', 41), ('percent', 40)]\n",
      "\n",
      "top occurences\n",
      "[('year', 0.56), ('scient', 0.49), ('spac', 0.44), ('work', 0.43), ('thursday', 0.41), ('research', 0.37), ('offic', 0.37), ('day', 0.36), ('mission', 0.34), ('control', 0.34), ('told', 0.33), ('month', 0.33), ('report', 0.31), ('station', 0.31), ('time', 0.31), ('stat', 0.3), ('early', 0.3), ('nasa', 0.3), ('cent', 0.3), ('plan', 0.3)]\n",
      "\n",
      "\n",
      "SHARE LISTINGS\n",
      "top word counts\n",
      "[('shar', 740), ('compan', 392), ('list', 347), ('million', 296), ('stock', 282), ('trad', 252), ('exchang', 238), ('pric', 209), ('percent', 200), ('market', 195), ('issu', 133), ('newsroom', 130), ('year', 128), ('group', 123), ('bank', 120), ('invest', 118), ('stat', 117), ('kong', 115), ('billion', 113), ('suspend', 110)]\n",
      "\n",
      "top occurences\n",
      "[('shar', 0.85), ('compan', 0.74), ('stock', 0.72), ('exchang', 0.64), ('newsroom', 0.59), ('million', 0.54), ('list', 0.52), ('trad', 0.5), ('market', 0.41), ('pric', 0.4), ('percent', 0.38), ('stat', 0.36), ('public', 0.34), ('issu', 0.32), ('year', 0.3), ('invest', 0.29), ('capit', 0.28), ('offer', 0.27), ('group', 0.27), ('tuesday', 0.24)]\n",
      "\n",
      "\n",
      "DOMESTIC MARKETS\n",
      "top word counts\n",
      "[('import', 541), ('ton', 324), ('percent', 265), ('year', 262), ('oil', 191), ('trad', 190), ('chin', 186), ('export', 161), ('million', 152), ('produc', 140), ('pric', 140), ('offic', 134), ('stat', 116), ('japan', 100), ('month', 99), ('market', 99), ('minist', 89), ('industr', 80), ('eu', 80), ('countr', 79)]\n",
      "\n",
      "top occurences\n",
      "[('import', 0.88), ('year', 0.62), ('ton', 0.57), ('percent', 0.48), ('trad', 0.47), ('produc', 0.45), ('offic', 0.44), ('export', 0.43), ('month', 0.42), ('newsroom', 0.41), ('million', 0.39), ('stat', 0.38), ('minist', 0.37), ('market', 0.35), ('industr', 0.32), ('countr', 0.32), ('total', 0.31), ('pric', 0.3), ('told', 0.29), ('domest', 0.28)]\n",
      "\n",
      "\n",
      "MONEY MARKETS\n",
      "top word counts\n",
      "[('bank', 4447), ('rate', 3772), ('percent', 3515), ('dollar', 3387), ('market', 2901), ('trad', 2490), ('day', 1775), ('deal', 1707), ('cent', 1533), ('currenc', 1472), ('mark', 1400), ('week', 1265), ('bill', 1246), ('clos', 1232), ('month', 1212), ('high', 1134), ('year', 1065), ('yen', 1057), ('econom', 1036), ('billion', 1012)]\n",
      "\n",
      "top occurences\n",
      "[('bank', 0.75), ('rate', 0.67), ('market', 0.62), ('percent', 0.58), ('newsroom', 0.53), ('trad', 0.49), ('dollar', 0.48), ('cent', 0.45), ('day', 0.41), ('deal', 0.37), ('currenc', 0.36), ('clos', 0.33), ('week', 0.33), ('month', 0.3), ('high', 0.29), ('low', 0.28), ('year', 0.28), ('billion', 0.28), ('end', 0.27), ('monday', 0.27)]\n",
      "\n",
      "total words: 777\n"
     ]
    }
   ],
   "source": [
    "#preliminary analysis\n",
    "\n",
    "\n",
    "def sort_dict(d):\n",
    "    return {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse = True)}\n",
    "\n",
    "def add_col(df, name):\n",
    "    # add a new column with zeroes\n",
    "    df[name] = pd.Series(0, index=df.index)\n",
    "    return df\n",
    "\n",
    "def top_entries(d, n):\n",
    "    # return n top entries in dictionary\n",
    "    d = sort_dict(d)\n",
    "    return list(d.items())[:20]\n",
    "\n",
    "def normalize_occurences(word_occurences, articles):\n",
    "    # convert total occurences to percentage occurence\n",
    "    return {key : round(value / articles.shape[0], 2) for (key, value) in word_occurences.items()}\n",
    "\n",
    "def get_top_words(data, topic, n):\n",
    "    articles = data[data['topic'] == topic]\n",
    "    \n",
    "    word_count = {} #map each word to how often it appears in the topic's articles\n",
    "    \n",
    "    word_occurences = {} # map each word to how many articles it appears in\n",
    "    \n",
    "    unique_words = set()\n",
    "    \n",
    "    for i in articles.index:\n",
    "        words = data['article_words'][i].split(',')\n",
    "        \n",
    "        for word in words:\n",
    "            word_count.setdefault(word, 0)\n",
    "            word_count[word] += 1\n",
    "            \n",
    "        unique_words = set(words)\n",
    "        unique_words = unique_words.union(unique_words)\n",
    "        \n",
    "        for word in unique_words:\n",
    "            word_occurences.setdefault(word, 0)\n",
    "            word_occurences[word] += 1\n",
    "    \n",
    "    word_occurences = normalize_occurences(word_occurences, articles)\n",
    "            \n",
    "    return top_entries(word_count, n), top_entries(word_occurences, n), unique_words\n",
    "\n",
    "topics = set(data.topic.values)\n",
    "\n",
    "all_unique_words = set()\n",
    "\n",
    "# getting the top words by topic\n",
    "for topic in topics:\n",
    "    top_words_by_count, top_words_by_occurences, unique_words = get_top_words(data, topic, 20)\n",
    "    \n",
    "    all_unique_words = all_unique_words.union(unique_words)\n",
    "    \n",
    "    print()\n",
    "    print(topic)\n",
    "    print(\"top word counts\")\n",
    "    print(top_words_by_count)\n",
    "\n",
    "    print()\n",
    "    print(\"top occurences\")\n",
    "    print(top_words_by_occurences)\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "print(\"total words:\", len(all_unique_words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_number</th>\n",
       "      <th>article_words</th>\n",
       "      <th>topic</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>open,absent,cent,cent,cent,stock,inflow,rate,k...</td>\n",
       "      <td>FOREX MARKETS</td>\n",
       "      <td>(0, 293465)\\t0.10372522328662358\\n  (0, 2111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>morn,stead,end,end,day,day,day,patch,patch,pat...</td>\n",
       "      <td>MONEY MARKETS</td>\n",
       "      <td>(0, 293465)\\t0.10372522328662358\\n  (0, 2111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>socc,socc,world,world,recent,law,fifa,fifa,fif...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>(0, 293465)\\t0.10372522328662358\\n  (0, 2111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>open,forint,forint,forint,forint,cent,cent,ste...</td>\n",
       "      <td>FOREX MARKETS</td>\n",
       "      <td>(0, 293465)\\t0.10372522328662358\\n  (0, 2111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>morn,complet,weekend,minut,minut,minut,arrow,d...</td>\n",
       "      <td>IRRELEVANT</td>\n",
       "      <td>(0, 293465)\\t0.10372522328662358\\n  (0, 2111...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_number                                      article_words  \\\n",
       "0               1  open,absent,cent,cent,cent,stock,inflow,rate,k...   \n",
       "1               2  morn,stead,end,end,day,day,day,patch,patch,pat...   \n",
       "2               3  socc,socc,world,world,recent,law,fifa,fifa,fif...   \n",
       "3               4  open,forint,forint,forint,forint,cent,cent,ste...   \n",
       "4               5  morn,complet,weekend,minut,minut,minut,arrow,d...   \n",
       "\n",
       "           topic                                             vector  \n",
       "0  FOREX MARKETS    (0, 293465)\\t0.10372522328662358\\n  (0, 2111...  \n",
       "1  MONEY MARKETS    (0, 293465)\\t0.10372522328662358\\n  (0, 2111...  \n",
       "2         SPORTS    (0, 293465)\\t0.10372522328662358\\n  (0, 2111...  \n",
       "3  FOREX MARKETS    (0, 293465)\\t0.10372522328662358\\n  (0, 2111...  \n",
       "4     IRRELEVANT    (0, 293465)\\t0.10372522328662358\\n  (0, 2111...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pre-processing\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# Vectorization parameters\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "def shuffle(data):\n",
    "    # reorder the data randomly, Google recommends this as a best practice\n",
    "    return data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "def vectorize(data):\n",
    "    v = TfidfVectorizer(\n",
    "            preprocessor = lambda x: x, #the preprocessor is set to be the identity function (it does nothing)\n",
    "            tokenizer = lambda x: x.split(','), #the tokenizer (which converts a string into individual words) splits a string at ','\n",
    "            ngram_range = NGRAM_RANGE,\n",
    "            token_mode = TOKEN_MODE\n",
    "            min_df - MIN_DOCUMENT_FREQUENCY) # we decide to use unigrams and bigrams as the google guide suggests\n",
    "    x_train =  v.fit_transform(data['article_words'])\n",
    "    \n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train)\n",
    "\n",
    "# data = add_spaces(data)\n",
    "def select_best_features(data):\n",
    "    \n",
    "    \n",
    "\n",
    "data = shuffle(data)    \n",
    "x_train = vectorize(data)\n",
    "data = select_best_features(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8d4e7431c20d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "print(data.head())\n",
    "print(pd.get_dummies(data, columns=['topic']).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(data):\n",
    "    #convert comma-seperated string into indidual words aka tokens\n",
    "    data['tokens'] = data['article_words'].map(lambda str: str.split(','))\n",
    "    return data\n",
    "\n",
    "def add_n_grams(data, n):\n",
    "    # create a new column of n_grams of custom length\n",
    "    # n_grams are a series of adjacent words of length n\n",
    "    # e.g: The three grams of the sentence 'The mouse ran up the clock' are\n",
    "    # 'The mouse ran', 'mouse ran up', 'ran up the', and 'up the clock'\n",
    "    data[str(n) + \"_grams\"] = data['tokens'].map(lambda tokens: n_grams(tokens, n))\n",
    "    return data\n",
    "    \n",
    "def n_grams(tokens, n):\n",
    "    # get n grams from a list of tokens\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens) + 1 - n)]\n",
    "\n",
    "def vectorize_n_gram(data, n_gram_name):\n",
    "    \n",
    "    \n",
    "    return data\n",
    "\n",
    "def to_vector(n_grams, codes):\n",
    "    pass\n",
    "\n",
    "def vectorize(data):\n",
    "    tf = TfidfTransformer()\n",
    "    print(tf.fit_transform(data['text']))\n",
    "    return data\n",
    "\n",
    "def gen_code_map(n_grams):\n",
    "    codes = {}\n",
    "    \n",
    "    for i in range(n_grams.shape[0]):\n",
    "        for n_gram in n_grams[i]:\n",
    "            if n_gram not in codes:\n",
    "                codes[n_gram] = len(codes)\n",
    "    \n",
    "    return codes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
